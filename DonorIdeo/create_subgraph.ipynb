{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing the graph size while having all donors and politicians in the same component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from DonorIdeo.config import ASSETS_DIR, DATABASE_PATH, TEMPORARY_DATA_DIR\n",
    "from DonorIdeo.littlesis_graph_utils import build_littlesis_graph, collect_donations_to\n",
    "import json\n",
    "from DonorIdeo.config import ASSETS_DIR, DATABASE_PATH, TEMPORARY_DATA_DIR\n",
    "from DonorIdeo.littlesis_graph_utils import build_littlesis_graph, collect_donations_to\n",
    "import json\n",
    "import networkx as nx\n",
    "from typing import List, Set, Tuple\n",
    "from tqdm import tqdm\n",
    "from typing import Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/viktorduepedersen/Documents/github/DonorIdeo/data/sources/littlesis-entities.json\n",
      "Reading /Users/viktorduepedersen/Documents/github/DonorIdeo/data/sources/littlesis-relationships.json\n"
     ]
    }
   ],
   "source": [
    "# 1. collect the donations to the politicians in the graph\n",
    "# Build the graph\n",
    "_, G_largest = build_littlesis_graph()\n",
    "\n",
    "# Collect the donations to the politicians in the graph\n",
    "littlesis_ids: List[int] = (\n",
    "    pd.read_csv(DATABASE_PATH, dtype={\"littlesis\": \"Int64\"})[\"littlesis\"]\n",
    "    .dropna()\n",
    "    .values.tolist()\n",
    ")\n",
    "donations_to_politician: Dict[int, Dict[int, int]] = collect_donations_to(\n",
    "    politicians=littlesis_ids, graph=G_largest, save_total_to_database=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intial checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All node_ids are in the graph from the start.\n"
     ]
    }
   ],
   "source": [
    "nodes_that_needs_to_be_there = set()\n",
    "# check that the politicians are in the graph\n",
    "for politician in donations_to_politician:\n",
    "    assert politician in G_largest.nodes()\n",
    "    nodes_that_needs_to_be_there.add(politician)\n",
    "\n",
    "# same as previous but data from a different source. Confusion about the ids\n",
    "for littlesis_id in littlesis_ids:\n",
    "    assert littlesis_id in G_largest.nodes()\n",
    "    nodes_that_needs_to_be_there.add(littlesis_id)\n",
    "\n",
    "# check that the donors are in the graph\n",
    "for littlesis_id in donations_to_politician:\n",
    "    for donor_id in donations_to_politician[littlesis_id]:\n",
    "        assert (\n",
    "            donor_id in G_largest.nodes()\n",
    "        ), f\"{donor_id} (donated to {littlesis_id}) not in graph\"\n",
    "        nodes_that_needs_to_be_there.add(donor_id)\n",
    "\n",
    "# Ensure all node_ids are in the graph from the start\n",
    "for node in nodes_that_needs_to_be_there:\n",
    "    if node not in G_largest:\n",
    "        raise ValueError(f\"Node {node} not in graph.\")\n",
    "print(\"All node_ids are in the graph from the start.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking out the component with all the nodes in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def component_with_all_nodes(\n",
    "    G: nx.Graph, node_ids_to_keep: Set[int]\n",
    ") -> nx.Graph | None:\n",
    "    # Find the connected component containing all node_ids\n",
    "    sorted_components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    for component in sorted_components:\n",
    "        if all(node in component for node in node_ids_to_keep):\n",
    "            return G.subgraph(component)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_component = component_with_all_nodes(G_largest, nodes_that_needs_to_be_there)\n",
    "assert (\n",
    "    valid_component is not None\n",
    "), \"Not all node_ids are in a connected component from the start.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we simply only include nodes that we need and they are still in a connected component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the graph connected? -> False\n"
     ]
    }
   ],
   "source": [
    "test_G = nx.subgraph(valid_component, nodes_that_needs_to_be_there)\n",
    "print(f\"Is the graph connected? -> {nx.is_connected(test_G)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_components = sorted(nx.connected_components(test_G), key=len, reverse=True)\n",
    "\n",
    "# lets make sure that these components haven't received any donations\n",
    "nodes_with_no_donations = set()\n",
    "for component in sorted_components:\n",
    "    if len(component) == 1:\n",
    "        for node_in_comp in component:\n",
    "            node_id: int = G_largest.nodes[node_in_comp][\"id\"]\n",
    "            assert len(donations_to_politician[node_id]) == 0\n",
    "            nodes_with_no_donations.add(node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the nodes with no donations from nodes_that_needs_to_be_there\n",
    "nodes_that_needs_to_be_there = nodes_that_needs_to_be_there - nodes_with_no_donations\n",
    "\n",
    "valid_component = component_with_all_nodes(test_G, nodes_that_needs_to_be_there)\n",
    "\n",
    "# save the valid_component as adjacency list\n",
    "nx.write_adjlist(valid_component, TEMPORARY_DATA_DIR / \"valid_component.adjlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the graph connected? -> True\n",
      "Number of nodes in the graph: 41439\n",
      "Number of edges in the graph: 712705\n",
      "Number of nodes in G_largest: 354538\n",
      "Number of edges in G_largest: 1677720\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is the graph connected? -> {nx.is_connected(valid_component)}\")\n",
    "print(f\"Number of nodes in the graph: {valid_component.number_of_nodes()}\")\n",
    "print(f\"Number of edges in the graph: {valid_component.number_of_edges()}\")\n",
    "\n",
    "print(f\"Number of nodes in G_largest: {G_largest.number_of_nodes()}\")\n",
    "print(f\"Number of edges in G_largest: {G_largest.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate node_attributes.csv and network.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the donations to the politicians in the graph\n",
    "littlesis_ids: List[int] = (\n",
    "    pd.read_csv(DATABASE_PATH, dtype={\"littlesis\": \"Int64\"})[\"littlesis\"]\n",
    "    .dropna()\n",
    "    .values.tolist()\n",
    ")\n",
    "\n",
    "# Make sure that nodes_with_no_donations are removed from littlesis_ids\n",
    "littlesis_ids = list(set(littlesis_ids) - nodes_with_no_donations)\n",
    "\n",
    "donations_to_politician: Dict[int, Dict[int, int]] = collect_donations_to(\n",
    "    politicians=littlesis_ids, graph=valid_component, save_total_to_database=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from DonorIdeo.json_utils import save_json\n",
    "from DonorIdeo.config import MINIMUM_NVD_DIR\n",
    "\n",
    "\"\"\"BECAUSE OF TIME NEEDS \n",
    "the following code is simply copied from the nvd.py file and modified to work with the new graph\n",
    "\n",
    "Not good practice but it works.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_custom_id(G_largest: nx.Graph) -> Tuple[Dict[str, str], Dict[str, str]]:\n",
    "    \"\"\"Generate a custom ID for each node\"\"\"\n",
    "    littlesis_id_to_my_id = {}\n",
    "    my_id_to_littlesis_id = {}\n",
    "\n",
    "    for i, node_id in enumerate(G_largest.nodes()):\n",
    "        my_id_to_littlesis_id[str(i + 1)] = str(node_id)\n",
    "        littlesis_id_to_my_id[str(node_id)] = str(i + 1)\n",
    "\n",
    "    # Save the mappings to JSON files\n",
    "    save_json(\n",
    "        data=littlesis_id_to_my_id,\n",
    "        path=MINIMUM_NVD_DIR / \"id-mapping-littlesis-to-mine.json\",\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    save_json(\n",
    "        data=my_id_to_littlesis_id,\n",
    "        path=MINIMUM_NVD_DIR / \"id-mapping-mine-to-littlesis.json\",\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    return littlesis_id_to_my_id, my_id_to_littlesis_id\n",
    "\n",
    "\n",
    "def generate_network_csv(\n",
    "    littlesis_id_to_my_id: Dict[str, str], G_largest: nx.Graph\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate network.csv, which contains the edges of the graph in the format \"a,b.\"\n",
    "    Note that the IDs are custom-created.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(MINIMUM_NVD_DIR / \"minimum-network.csv\", \"w\") as f:\n",
    "        for edge in tqdm(G_largest.edges(), desc=\"Writing network.csv\"):\n",
    "            littlesis_src_id, littlesis_dst_id = edge\n",
    "\n",
    "            src_id = littlesis_id_to_my_id[str(littlesis_src_id)]\n",
    "            dst_id = littlesis_id_to_my_id[str(littlesis_dst_id)]\n",
    "\n",
    "            f.write(f\"{src_id},{dst_id}\\n\")\n",
    "\n",
    "\n",
    "def generate_node_attributes_csv(\n",
    "    donation_to_politician: Dict[int, Dict[int, int]],\n",
    "    my_id_to_littlesis_id: Dict[str, str],\n",
    ") -> None:\n",
    "    # Generate node_attributes.csv from donation_to_politician\n",
    "    outpath = MINIMUM_NVD_DIR / \"minimum-node_attributes.csv\"\n",
    "    with open(outpath, \"w\") as f:\n",
    "        # Write the header\n",
    "        f.write(\"donor,\")\n",
    "        politician_ids: List[int] = [\n",
    "            politician_id for politician_id in set(donation_to_politician.keys())\n",
    "        ]\n",
    "        f.write(\",\".join([str(polit) for polit in politician_ids]) + \"\\n\")\n",
    "\n",
    "        # Write the data\n",
    "        for my_donor_id, littlesis_donor_id in tqdm(\n",
    "            my_id_to_littlesis_id.items(), desc=outpath.name\n",
    "        ):\n",
    "            # Write the donor ID\n",
    "            line = f\"{my_donor_id},\"\n",
    "\n",
    "            for politician in politician_ids:\n",
    "                littlesis_donor_id: int = int(littlesis_donor_id)\n",
    "                if littlesis_donor_id in donation_to_politician[politician]:\n",
    "                    donation: int = donation_to_politician[politician][\n",
    "                        littlesis_donor_id\n",
    "                    ]\n",
    "\n",
    "                    if donation < 0:  # Negative donations are not allowed\n",
    "                        donation = 0\n",
    "                    logged_donation: int = np.log(donation + 1)  # Add 1 to avoid log(0)\n",
    "                    line += f\"{logged_donation},\"\n",
    "                else:\n",
    "                    line += \"0,\"  # np.log(1) = 0\n",
    "            f.write(line[:-1] + \"\\n\")  # Remove the last comma\n",
    "\n",
    "\n",
    "def compute_distance_matrix(write_to_file: bool = True) -> np.ndarray:\n",
    "    node_attributes = pd.read_csv(MINIMUM_NVD_DIR / \"minimum-node_attributes.csv\")\n",
    "\n",
    "    politician_ids = node_attributes.columns[1:]\n",
    "    politician_matrix_id = {\n",
    "        int(polit_id): i for i, polit_id in enumerate(politician_ids)\n",
    "    }\n",
    "    distance_matrix = np.zeros((len(politician_ids), len(politician_ids)))\n",
    "    for polit_id in tqdm(politician_ids, desc=\"Building distance matrix\"):\n",
    "        specific_path = MINIMUM_NVD_DIR / \"nvd\" / f\"{polit_id}.csv\"\n",
    "        distances = pd.read_csv(\n",
    "            specific_path, skiprows=1, names=[\"src\", \"dst\", \"distance\"]\n",
    "        )\n",
    "        # Setting all values\n",
    "        for idx, row in distances.iterrows():\n",
    "            src_matrix_id = int(politician_matrix_id[row[\"src\"]])\n",
    "            dst_matrix_id = int(politician_matrix_id[row[\"dst\"]])\n",
    "            dist = row[\"distance\"]\n",
    "\n",
    "            # set value in matrix\n",
    "            distance_matrix[src_matrix_id, dst_matrix_id] = dist\n",
    "            distance_matrix[dst_matrix_id, src_matrix_id] = dist\n",
    "\n",
    "    if write_to_file:\n",
    "        outpath = MINIMUM_NVD_DIR / \"distance_matrix.csv\"\n",
    "        print(f\"Writing distance matrix to file: {outpath}\")\n",
    "        np.savetxt(outpath, distance_matrix, delimiter=\",\")\n",
    "\n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def project_and_scale(distance_matrix: np.ndarray, model: TSNE) -> np.ndarray:\n",
    "    X_embedded = model.fit_transform(distance_matrix)\n",
    "\n",
    "    # Scale the values to be between -1 and 1\n",
    "    X_embedded = (X_embedded - X_embedded.min()) / (X_embedded.max() - X_embedded.min())\n",
    "    X_embedded = (X_embedded * 2) - 1\n",
    "\n",
    "    return X_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving /Users/viktorduepedersen/Documents/github/DonorIdeo/data/minimum_nvd/id-mapping-littlesis-to-mine.json\n",
      "Saving /Users/viktorduepedersen/Documents/github/DonorIdeo/data/minimum_nvd/id-mapping-mine-to-littlesis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing network.csv: 100%|██████████| 712705/712705 [00:02<00:00, 356141.50it/s]\n",
      "minimum-node_attributes.csv: 100%|██████████| 41439/41439 [00:04<00:00, 8374.16it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 2. Build the network.csv and node_attributes.csv\n",
    "\n",
    "Firstly I need to create a new_id for each of the nodes in the biggest connected components.\n",
    "This is because the julia script requires the nodes to be numbered from 1 to n.\n",
    "\"\"\"\n",
    "littlesis_id_to_my_id, my_id_to_littlesis_id = generate_custom_id(valid_component)\n",
    "\n",
    "generate_network_csv(littlesis_id_to_my_id, valid_component)\n",
    "\n",
    "generate_node_attributes_csv(\n",
    "    donation_to_politician=donations_to_politician,\n",
    "    my_id_to_littlesis_id=my_id_to_littlesis_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the minimum-nvd.jl script can be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#  3. Compute the distance matrix\n",
    "assert (\n",
    "    len(\n",
    "        [\n",
    "            politician_file\n",
    "            for politician_file in os.listdir(MINIMUM_NVD_DIR / \"nvd\")\n",
    "            if politician_file.endswith(\".csv\")\n",
    "        ]\n",
    "    )\n",
    "    == 551\n",
    "), f\"The nvd data directory is missing some files. The Julia script are most likely not finished running.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building distance matrix: 100%|██████████| 551/551 [00:03<00:00, 167.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing distance matrix to file: /Users/viktorduepedersen/Documents/github/DonorIdeo/data/minimum_nvd/distance_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "distance_matrix = compute_distance_matrix(write_to_file=True)\n",
    "\n",
    "# Project the distance matrix to 2D and scale the values to -1 to 1\n",
    "tsne_2 = TSNE(n_components=2, metric=\"precomputed\", init=\"random\", random_state=42)\n",
    "X_embedded_2 = project_and_scale(distance_matrix=distance_matrix, model=tsne_2)\n",
    "\n",
    "# Project the distance matrix to 1D and scale the values to -1 to 1\n",
    "tsne_1 = TSNE(n_components=1, metric=\"precomputed\", init=\"random\", random_state=42)\n",
    "X_embedded_1 = project_and_scale(distance_matrix=distance_matrix, model=tsne_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv(DATABASE_PATH, dtype={\"littlesis\": \"Int64\"})\n",
    "\n",
    "node_attributes = pd.read_csv(MINIMUM_NVD_DIR / \"minimum-node_attributes.csv\")\n",
    "politician_ids = node_attributes.columns[1:]\n",
    "\n",
    "# zip the politicians with their projections\n",
    "politicians_with_projections = zip(politician_ids, X_embedded_1, X_embedded_2)\n",
    "\n",
    "# Add the projections to the database\n",
    "for littlesis_id, projection_1d, projection_2d in politicians_with_projections:\n",
    "    database.loc[\n",
    "        database[\"littlesis\"] == int(littlesis_id), f\"minimum-projection-1d\"\n",
    "    ] = projection_1d\n",
    "    database.loc[\n",
    "        database[\"littlesis\"] == int(littlesis_id), f\"minimum-projection-2d_x\"\n",
    "    ] = projection_2d[0]\n",
    "    database.loc[\n",
    "        database[\"littlesis\"] == int(littlesis_id), f\"minimum-projection-2d_y\"\n",
    "    ] = projection_2d[1]\n",
    "\n",
    "# Save the database\n",
    "database.to_csv(DATABASE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
